<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p2" style="overflow: hidden; position: relative; background-color: white; width: 742px; height: 1100px;">
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<style type="text/css" >

#t1_2{left:70px;bottom:991px;letter-spacing:0.07px;}
#t2_2{left:472px;bottom:991px;letter-spacing:0.03px;word-spacing:0.09px;}
#t3_2{left:70px;bottom:949px;letter-spacing:0.09px;word-spacing:-0.19px;}
#t4_2{left:70px;bottom:930px;letter-spacing:0.07px;word-spacing:-0.96px;}
#t5_2{left:70px;bottom:912px;letter-spacing:0.08px;word-spacing:0.07px;}
#t6_2{left:70px;bottom:894px;letter-spacing:0.09px;word-spacing:-0.02px;}
#t7_2{left:70px;bottom:875px;letter-spacing:0.09px;word-spacing:-0.01px;}
#t8_2{left:576px;bottom:883px;}
#t9_2{left:586px;bottom:875px;letter-spacing:0.08px;word-spacing:0.01px;}
#ta_2{left:69px;bottom:857px;letter-spacing:0.09px;word-spacing:-0.06px;}
#tb_2{left:247px;bottom:865px;}
#tc_2{left:257px;bottom:857px;letter-spacing:0.09px;word-spacing:-0.06px;}
#td_2{left:69px;bottom:839px;letter-spacing:0.11px;}
#te_2{left:93px;bottom:846px;}
#tf_2{left:103px;bottom:839px;letter-spacing:0.1px;word-spacing:-0.01px;}
#tg_2{left:286px;bottom:846px;}
#th_2{left:295px;bottom:839px;letter-spacing:0.07px;word-spacing:0.03px;}
#ti_2{left:85px;bottom:821px;letter-spacing:0.05px;word-spacing:1.15px;}
#tj_2{left:360px;bottom:821px;letter-spacing:0.09px;word-spacing:1.07px;}
#tk_2{left:430px;bottom:821px;}
#tl_2{left:438px;bottom:821px;}
#tm_2{left:444px;bottom:821px;letter-spacing:0.1px;}
#tn_2{left:473px;bottom:821px;letter-spacing:0.09px;word-spacing:1.06px;}
#to_2{left:70px;bottom:802px;letter-spacing:0.09px;word-spacing:0.55px;}
#tp_2{left:70px;bottom:784px;letter-spacing:0.09px;word-spacing:-0.02px;}
#tq_2{left:70px;bottom:766px;letter-spacing:0.09px;word-spacing:0.64px;}
#tr_2{left:70px;bottom:748px;letter-spacing:0.09px;word-spacing:0.05px;}
#ts_2{left:70px;bottom:729px;letter-spacing:0.08px;}
#tt_2{left:70px;bottom:711px;letter-spacing:0.07px;word-spacing:0.04px;}
#tu_2{left:69px;bottom:693px;letter-spacing:0.1px;word-spacing:0.05px;}
#tv_2{left:240px;bottom:693px;}
#tw_2{left:248px;bottom:693px;letter-spacing:0.07px;word-spacing:0.11px;}
#tx_2{left:70px;bottom:675px;letter-spacing:0.09px;word-spacing:0.03px;}
#ty_2{left:70px;bottom:656px;letter-spacing:0.09px;word-spacing:-0.08px;}
#tz_2{left:70px;bottom:638px;letter-spacing:0.09px;word-spacing:0.02px;}
#t10_2{left:70px;bottom:620px;letter-spacing:0.09px;}
#t11_2{left:194px;bottom:620px;letter-spacing:0.08px;word-spacing:0.01px;}
#t12_2{left:85px;bottom:602px;letter-spacing:0.09px;word-spacing:0.05px;}
#t13_2{left:70px;bottom:583px;letter-spacing:0.08px;word-spacing:-0.8px;}
#t14_2{left:70px;bottom:565px;letter-spacing:0.08px;word-spacing:0.7px;}
#t15_2{left:70px;bottom:547px;letter-spacing:0.09px;word-spacing:0.02px;}
#t16_2{left:70px;bottom:528px;letter-spacing:0.09px;word-spacing:1.15px;}
#t17_2{left:70px;bottom:510px;letter-spacing:0.09px;word-spacing:-0.05px;}
#t18_2{left:70px;bottom:492px;letter-spacing:0.09px;word-spacing:0.16px;}
#t19_2{left:70px;bottom:474px;letter-spacing:0.09px;word-spacing:0.03px;}
#t1a_2{left:70px;bottom:455px;letter-spacing:0.08px;word-spacing:0.78px;}
#t1b_2{left:70px;bottom:437px;letter-spacing:0.08px;word-spacing:-0.98px;}
#t1c_2{left:70px;bottom:419px;letter-spacing:0.09px;word-spacing:0.04px;}
#t1d_2{left:70px;bottom:401px;letter-spacing:0.08px;word-spacing:0.03px;}
#t1e_2{left:85px;bottom:382px;letter-spacing:0.08px;word-spacing:-0.03px;}
#t1f_2{left:70px;bottom:364px;letter-spacing:0.09px;word-spacing:0.06px;}
#t1g_2{left:70px;bottom:346px;letter-spacing:0.09px;word-spacing:0.08px;}
#t1h_2{left:69px;bottom:328px;letter-spacing:0.07px;word-spacing:0.09px;}
#t1i_2{left:70px;bottom:309px;letter-spacing:0.08px;word-spacing:-0.03px;}
#t1j_2{left:70px;bottom:291px;letter-spacing:0.08px;word-spacing:-0.22px;}
#t1k_2{left:70px;bottom:273px;letter-spacing:0.07px;word-spacing:-0.12px;}
#t1l_2{left:85px;bottom:254px;letter-spacing:0.06px;word-spacing:-0.37px;}
#t1m_2{left:485px;bottom:254px;letter-spacing:0.1px;word-spacing:-0.43px;}
#t1n_2{left:599px;bottom:254px;letter-spacing:0.07px;word-spacing:-0.37px;}
#t1o_2{left:70px;bottom:236px;letter-spacing:0.08px;word-spacing:-0.66px;}
#t1p_2{left:196px;bottom:236px;letter-spacing:0.08px;word-spacing:-0.65px;}
#t1q_2{left:70px;bottom:218px;letter-spacing:0.08px;word-spacing:0.08px;}
#t1r_2{left:70px;bottom:200px;letter-spacing:0.09px;word-spacing:0.56px;}
#t1s_2{left:70px;bottom:181px;letter-spacing:0.08px;word-spacing:-0.02px;}
#t1t_2{left:70px;bottom:163px;letter-spacing:0.09px;word-spacing:-0.02px;}
#t1u_2{left:70px;bottom:143px;}
#t1v_2{left:75px;bottom:137px;letter-spacing:0.07px;}
#t1w_2{left:70px;bottom:128px;}
#t1x_2{left:75px;bottom:122px;letter-spacing:0.1px;}
#t1y_2{left:70px;bottom:113px;}
#t1z_2{left:75px;bottom:107px;letter-spacing:0.06px;}
#t20_2{left:70px;bottom:97px;}
#t21_2{left:75px;bottom:92px;letter-spacing:0.07px;}
#t22_2{left:70px;bottom:55px;letter-spacing:0.04px;word-spacing:0.29px;}

.s0_2{font-size:12px;font-family:LinBiolinumT_g-;color:#000;}
.s1_2{font-size:15px;font-family:LinLibertineT_h3;color:#000;}
.s2_2{font-size:11px;font-family:LinLibertineT_h3;color:#781D7D;}
.s3_2{font-size:15px;font-family:LinLibertineT_h3;color:#781D7D;}
.s4_2{font-size:15px;font-family:LinLibertineTI_hf;color:#000;}
.s5_2{font-size:9px;font-family:LinLibertineT_h3;color:#000;}
.s6_2{font-size:12px;font-family:LinLibertineT_h3;color:#005596;}
.s7_2{font-size:12px;font-family:LinLibertineT_h3;color:#000;}
.t.v0_2{transform:scaleX(0.979);}
.t.v1_2{transform:scaleX(0.991);}
.t.v2_2{transform:scaleX(1.018);}
.t.v3_2{transform:scaleX(1.02);}
.t.v4_2{transform:scaleX(0.984);}
.t.v5_2{transform:scaleX(1.005);}
.t.v6_2{transform:scaleX(1.019);}
.t.v7_2{transform:scaleX(0.993);}
.t.v8_2{transform:scaleX(0.994);}
.t.v9_2{transform:scaleX(1.013);}
.t.v10_2{transform:scaleX(1.011);}
.t.v11_2{transform:scaleX(0.983);}
.t.v12_2{transform:scaleX(1.008);}
</style>
<style id="fonts2" type="text/css" >

@font-face {
	font-family: LinBiolinumT_g-;
	src: url("fonts/LinBiolinumT_g-.woff") format("woff");
}

@font-face {
	font-family: LinLibertineTI_hf;
	src: url("fonts/LinLibertineTI_hf.woff") format("woff");
}

@font-face {
	font-family: LinLibertineT_h3;
	src: url("fonts/LinLibertineT_h3.woff") format("woff");
}

</style>
<div id="pg2Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg2" style="-webkit-user-select: none;"><svg id="pdf2" width="742" height="1100" viewBox="0 0 742 1100" style="width:742px; height:1100px; z-index: 0; isolation: isolate;" version="1.1" xmlns="http://www.w3.org/2000/svg">
<defs>
<style>
.g0_2{fill:none;stroke:#000;stroke-width:0.608;stroke-miterlimit:10;}
</style>
</defs>
<path d="M70 942.3h73.1" class="g0_2"/>
</svg></div>
<div class="text-container"><span id="t1_2" class="t s0_2">187:2 </span><span id="t2_2" class="t s0_2">Tom Yuviler and Dana Drachsler-Cohen </span>
<span id="t3_2" class="t v0_2 s1_2" data-mappings='[[78,"fi"]]'>generate candidate adversarial examples and submit them to the network, until ﬁnding a successful </span>
<span id="t4_2" class="t v0_2 s1_2">candidate. However, existing approaches require a very high number of queries, making their attacks </span>
<span id="t5_2" class="t v1_2 s1_2">very expensive, or even infeasible, in a real-world setting where the network limits the number of </span>
<span id="t6_2" class="t v0_2 s1_2" data-mappings='[[40,"fi"]]'>queries. For example, many online classiﬁers allow the user to pose a limited number of queries for </span>
<span id="t7_2" class="t s1_2">free per month, after which users can pay for more queries: Amazon Rekognition </span>
<span id="t8_2" class="t s2_2">1 </span>
<span id="t9_2" class="t s1_2">and Microsoft </span>
<span id="ta_2" class="t v2_2 s1_2">Azure Computer Vision API </span>
<span id="tb_2" class="t s2_2">2 </span>
<span id="tc_2" class="t v2_2 s1_2">allow 5000 queries for free, while Clarifai moderation-recognition </span>
<span id="td_2" class="t s1_2">API </span>
<span id="te_2" class="t s2_2">3 </span>
<span id="tf_2" class="t s1_2">and Google Cloud Vision API </span>
<span id="tg_2" class="t s2_2">4 </span>
<span id="th_2" class="t s1_2">allow 1000 queries for free. </span>
<span id="ti_2" class="t v3_2 s1_2">To make few pixel attacks more practical, </span><span id="tj_2" class="t v3_2 s3_2">Croce et al</span><span id="tk_2" class="t s3_2">. </span><span id="tl_2" class="t v3_2 s1_2">[</span><span id="tm_2" class="t v3_2 s3_2">2022</span><span id="tn_2" class="t v3_2 s1_2">] propose the Sparse-RS attack, </span>
<span id="to_2" class="t v3_2 s1_2">relying on random search, where the attacker is limited in the number of queries posed to the </span>
<span id="tp_2" class="t s1_2">network. For one pixel attacks, Sparse-RS generates candidate adversarial examples by randomly </span>
<span id="tq_2" class="t v3_2 s1_2">picking a pixel to perturb and a perturbation. Sparse-RS obtains a state-of-the-art success rate </span>
<span id="tr_2" class="t v4_2 s1_2" data-mappings='[[88,"fi"]]'>(where the success rate is the percentage of perturbed images that the network misclassiﬁes out of </span>
<span id="ts_2" class="t v5_2 s1_2">all perturbed images) with a few thousand queries. However, in practical settings, this number is </span>
<span id="tt_2" class="t s1_2">still high. To illustrate, our experiments show that Sparse-RS obtains at best a success rate of 51% </span>
<span id="tu_2" class="t v3_2 s1_2">with 1000 queries (Section </span><span id="tv_2" class="t v3_2 s3_2">6</span><span id="tw_2" class="t v3_2 s1_2">). Namely, given a 5000 query limit (as in Amazon Rekognition and </span>
<span id="tx_2" class="t s1_2">Microsoft Azure Computer Vision API), Sparse-RS can generate about three adversarial examples </span>
<span id="ty_2" class="t v6_2 s1_2">for free per month, and given a 1000 query limit (as in Clarifai moderation-recognition API and </span>
<span id="tz_2" class="t v7_2 s1_2">Google Cloud Vision API), it can generate about one adversarial example for free per month. This </span>
<span id="t10_2" class="t s1_2">raises the question: </span><span id="t11_2" class="t s4_2">Can we compute one pixel attacks with a few hundred queries? </span>
<span id="t12_2" class="t v3_2 s1_2">In this work, we draw inspiration from program synthesis and propose to compute one pixel </span>
<span id="t13_2" class="t v0_2 s1_2" data-mappings='[[43,"fi"]]'>adversarial programs. Given an image classiﬁer and a training set of images, our goal is to synthesize </span>
<span id="t14_2" class="t v3_2 s1_2" data-mappings='[[30,"fi"]]'>a program that, given a classiﬁer and an image to attack, generates an adversarial example by </span>
<span id="t15_2" class="t v8_2 s1_2">dynamically identifying the most prominent pixel locations and perturbations for the attack. Like </span>
<span id="t16_2" class="t v3_2 s1_2">prior one pixel attacks, an adversarial program generates candidate adversarial examples and </span>
<span id="t17_2" class="t v9_2 s1_2">submits them to the network. Unlike prior one pixel attacks, the large number of queries is only </span>
<span id="t18_2" class="t v3_2 s1_2" data-mappings='[[90,"fi"]]'>required for the synthesis process. Afterwards, the synthesized program dynamically identiﬁes </span>
<span id="t19_2" class="t v1_2 s1_2">the prominent candidate adversarial examples, based on the learned conditions and the network’s </span>
<span id="t1a_2" class="t v3_2 s1_2">output for the submitted candidates. Conceptually, the synthesized program provides a query- </span>
<span id="t1b_2" class="t v0_2 s1_2" data-mappings='[[1,"ffi"]]'>eﬃcient counterexample-guided attack. Our adversarial programs are more practical than Sparse-RS: </span>
<span id="t1c_2" class="t v0_2 s1_2">the attacker only needs to invest an initial fee for the synthesis, and then she can run the attack on </span>
<span id="t1d_2" class="t s1_2">new inputs with an order of magnitude fewer queries. </span>
<span id="t1e_2" class="t v10_2 s1_2">As in any program synthesis task, computing adversarial programs introduces two main chal- </span>
<span id="t1f_2" class="t v0_2 s1_2" data-mappings='[[59,"fi"]]'>lenges: (1) identifying a small yet expressive domain-speciﬁc language (DSL) for the programs, and </span>
<span id="t1g_2" class="t v11_2 s1_2" data-mappings='[[18,"ffi"],[45,"fi"]]'>(2) designing an eﬃcient search algorithm to ﬁnd a program in this DSL meeting the requirements. </span>
<span id="t1h_2" class="t v1_2 s1_2">Additionally, computing adversarial programs introduces unique challenges: (3) the program does </span>
<span id="t1i_2" class="t v0_2 s1_2" data-mappings='[[52,"fi"]]'>not only depend on the given input (i.e., the classiﬁer and the image to attack), but also on interme- </span>
<span id="t1j_2" class="t v0_2 s1_2">diate results (i.e., the candidate adversarial examples) and (4) candidate examples do not adhere to a </span>
<span id="t1k_2" class="t v0_2 s1_2">strict partial order, thus an unsuccessful candidate does not enable to safely prune other candidates. </span>
<span id="t1l_2" class="t v0_2 s1_2">To cope with the above challenges, we propose a program sketch [</span><span id="t1m_2" class="t v0_2 s3_2">Solar-Lezama 2009</span><span id="t1n_2" class="t v0_2 s1_2">] expressing </span>
<span id="t1o_2" class="t v0_2 s4_2">prioritizing programs</span><span id="t1p_2" class="t v0_2 s1_2" data-mappings='[[16,"fi"]]'>. Given a classiﬁer and an image, a prioritizing program enumerates all possible </span>
<span id="t1q_2" class="t v3_2 s1_2" data-mappings='[[37,"fi"]]'>candidate adversarial examples until ﬁnding a successful adversarial example. That is, given an </span>
<span id="t1r_2" class="t v3_2 s1_2" data-mappings='[[49,"fi"]]'>unlimited number of queries, it is guaranteed to ﬁnd a successful candidate, if exists. The goal </span>
<span id="t1s_2" class="t v12_2 s1_2" data-mappings='[[83,"fi"]]'>of a prioritizing program is to dynamically prioritize the candidates, in order to ﬁnd a successful </span>
<span id="t1t_2" class="t s1_2">adversarial example with a minimal number of queries. The dynamic prioritization is determined </span>
<span id="t1u_2" class="t s5_2">1 </span>
<span id="t1v_2" class="t s6_2">https://aws.amazon.com/rekognition/pricing/?nc=sn&amp;loc=4 </span>
<span id="t1w_2" class="t s5_2">2 </span>
<span id="t1x_2" class="t s6_2">https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/ </span>
<span id="t1y_2" class="t s5_2">3 </span>
<span id="t1z_2" class="t s6_2">https://www.clarifai.com/pricing </span>
<span id="t20_2" class="t s5_2">4 </span>
<span id="t21_2" class="t s6_2">https://cloud.google.com/vision/pricing </span>
<span id="t22_2" class="t s7_2">Proc. ACM Program. Lang., Vol. 7, No. PLDI, Article 187. Publication date: June 2023. </span></div>

</div>
</body>
</html>
